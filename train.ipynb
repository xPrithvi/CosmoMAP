{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "\n",
    "# ✅ Step 1: Use np.memmap to load .npy files without RAM overload\n",
    "print(\"Mapping datasets to memory...\")\n",
    "X_mass = np.load(\"/home/amanbh/projects/tf217/DISS/Maps_Mcdm_IllustrisTNG_LH_z=0.00.npy\", mmap_mode='r')\n",
    "X_vel  = np.load(\"/home/amanbh/projects/tf217/DISS/Maps_Vcdm_IllustrisTNG_LH_z=0.00.npy\", mmap_mode='r')\n",
    "Y_gas  = np.load(\"/home/amanbh/projects/tf217/DISS/Maps_Mgas_IllustrisTNG_LH_z=0.00.npy\", mmap_mode='r')\n",
    "Y_temp = np.load(\"/home/amanbh/projects/tf217/DISS/Maps_T_IllustrisTNG_LH_z=0.00.npy\", mmap_mode='r')\n",
    "\n",
    "total_samples = len(X_mass)\n",
    "print(f\"Total samples: {total_samples}\")  # Check dataset size\n",
    "\n",
    "   # Pre-compute dataset statistics once (on a subset if needed)\n",
    "   def get_dataset_stats(data, n_samples=100):\n",
    "       min_vals = []\n",
    "       max_vals = []\n",
    "       for i in range(min(n_samples, len(data))):\n",
    "           min_vals.append(np.min(data[i]))\n",
    "           max_vals.append(np.max(data[i]))\n",
    "       return np.min(min_vals), np.max(max_vals)\n",
    "   \n",
    "   # Then normalize consistently across all batches\n",
    "   X_mass_min, X_mass_max = get_dataset_stats(X_mass)\n",
    "   # Apply these consistently in normalize_batch function\n",
    "\n",
    "# ✅ Step 2: Efficient Normalization (applied dynamically per batch)\n",
    "def normalize_batch(batch, epsilon=1e-6):\n",
    "    min_val = np.min(batch)\n",
    "    max_val = np.max(batch)\n",
    "    return np.log1p(batch - min_val + epsilon) / np.log1p(max_val - min_val + epsilon)\n",
    "\n",
    "# ✅ Step 3: Create a Memory-Efficient Data Generator with indices\n",
    "def data_generator(start, end):\n",
    "    for i in range(start, end):\n",
    "        X1 = normalize_batch(X_mass[i])  # Dark matter mass\n",
    "        X2 = normalize_batch(X_vel[i])   # Dark matter velocity\n",
    "        Y1 = normalize_batch(Y_gas[i])   # Gas density\n",
    "        Y2 = normalize_batch(Y_temp[i])  # Temperature\n",
    "        yield (np.stack([X1, X2], axis=-1), np.stack([Y1, Y2], axis=-1))\n",
    "\n",
    "# ✅ Step 4: Split indices for training, validation, and test\n",
    "train_fraction = 0.8\n",
    "train_samples = int(total_samples * train_fraction)\n",
    "remaining_samples = total_samples - train_samples\n",
    "# Split remaining samples equally for validation and test\n",
    "val_samples = int(remaining_samples * 0.5)\n",
    "test_samples = remaining_samples - val_samples\n",
    "\n",
    "print(f\"Training samples: {train_samples}, Validation samples: {val_samples}, Test samples: {test_samples}\")\n",
    "\n",
    "# ✅ Step 5: Create TensorFlow Datasets for Training, Validation, and Test\n",
    "train_dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: data_generator(0, train_samples),\n",
    "    output_signature=(\n",
    "        tf.TensorSpec(shape=(256, 256, 2), dtype=tf.float32),  # Adjust shape if needed\n",
    "        tf.TensorSpec(shape=(256, 256, 2), dtype=tf.float32)\n",
    "    )\n",
    ")\n",
    "train_dataset = train_dataset.shuffle(1000).batch(8).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: data_generator(train_samples, train_samples + val_samples),\n",
    "    output_signature=(\n",
    "        tf.TensorSpec(shape=(256, 256, 2), dtype=tf.float32),\n",
    "        tf.TensorSpec(shape=(256, 256, 2), dtype=tf.float32)\n",
    "    )\n",
    ")\n",
    "val_dataset = val_dataset.batch(8).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: data_generator(train_samples + val_samples, total_samples),\n",
    "    output_signature=(\n",
    "        tf.TensorSpec(shape=(256, 256, 2), dtype=tf.float32),\n",
    "        tf.TensorSpec(shape=(256, 256, 2), dtype=tf.float32)\n",
    "    )\n",
    ")\n",
    "test_dataset = test_dataset.batch(8).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# ✅ Step 6: Define U-Net Model (or any model you want)\n",
    "def unet_model(input_shape):\n",
    "    inputs = tf.keras.layers.Input(input_shape)\n",
    "\n",
    "    # Encoder\n",
    "    conv1 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same')(inputs)\n",
    "    conv1 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same')(conv1)\n",
    "    pool1 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "\n",
    "    conv2 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same')(pool1)\n",
    "    conv2 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same')(conv2)\n",
    "    pool2 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "\n",
    "    conv3 = tf.keras.layers.Conv2D(256, (3, 3), activation='relu', padding='same')(pool2)\n",
    "    conv3 = tf.keras.layers.Conv2D(256, (3, 3), activation='relu', padding='same')(conv3)\n",
    "    pool3 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "\n",
    "    # Bottleneck\n",
    "    conv4 = tf.keras.layers.Conv2D(512, (3, 3), activation='relu', padding='same')(pool3)\n",
    "    conv4 = tf.keras.layers.Conv2D(512, (3, 3), activation='relu', padding='same')(conv4)\n",
    "\n",
    "    # Decoder\n",
    "    up1 = tf.keras.layers.UpSampling2D(size=(2, 2))(conv4)\n",
    "    up1 = tf.keras.layers.Conv2D(256, (3, 3), activation='relu', padding='same')(up1)\n",
    "    merge1 = tf.keras.layers.Concatenate()([up1, conv3])\n",
    "\n",
    "    up2 = tf.keras.layers.UpSampling2D(size=(2, 2))(merge1)\n",
    "    up2 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same')(up2)\n",
    "    merge2 = tf.keras.layers.Concatenate()([up2, conv2])\n",
    "\n",
    "    up3 = tf.keras.layers.UpSampling2D(size=(2, 2))(merge2)\n",
    "    up3 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same')(up3)\n",
    "    merge3 = tf.keras.layers.Concatenate()([up3, conv1])\n",
    "\n",
    "    outputs = tf.keras.layers.Conv2D(2, (1, 1), activation='sigmoid')(merge3)\n",
    "\n",
    "    # Compile with MSE loss; also track MAE and Huber loss metrics.\n",
    "    model = tf.keras.models.Model(inputs, outputs)\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='mse',\n",
    "        metrics=[\n",
    "            'mae',\n",
    "            tf.keras.metrics.MeanMetricWrapper(tf.keras.losses.Huber(), name='huber_loss')\n",
    "        ]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Initialize the model\n",
    "model = unet_model((256, 256, 2))\n",
    "model.summary()\n",
    "\n",
    "# ✅ Step 7: Setup Checkpointing and Resume Training Capability\n",
    "checkpoint_dir = './checkpoints'\n",
    "checkpoint_path = os.path.join(checkpoint_dir, 'cp-{epoch:04d}.ckpt')\n",
    "\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir)\n",
    "\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_path,\n",
    "    verbose=1,\n",
    "    save_weights_only=False,  # Save full model\n",
    "    save_freq='epoch'\n",
    ")\n",
    "\n",
    "csv_logger = tf.keras.callbacks.CSVLogger('training_log.csv', append=True)\n",
    "\n",
    "# EarlyStopping callback: Stop training if validation loss doesn't improve for 10 epochs.\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Check if there is an existing checkpoint to resume from\n",
    "latest = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "initial_epoch = 0\n",
    "if latest:\n",
    "    print(\"Found checkpoint at\", latest)\n",
    "    model = tf.keras.models.load_model(latest)\n",
    "    initial_epoch = int(latest.split('-')[-1].split('.')[0])\n",
    "    print(f\"Resuming training from epoch {initial_epoch}\")\n",
    "\n",
    "# ✅ Step 8: Create a Callback for Live Plotting of Epoch vs Loss, MAE, and Huber Loss\n",
    "class LivePlotCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self):\n",
    "        super(LivePlotCallback, self).__init__()\n",
    "        self.epochs = []\n",
    "        self.losses = []\n",
    "        self.mae = []\n",
    "        self.huber = []\n",
    "        plt.ion()  # Enable interactive mode\n",
    "        # Create 3 subplots for Loss, MAE, and Huber loss\n",
    "        self.figure, (self.ax1, self.ax2, self.ax3) = plt.subplots(1, 3, figsize=(18, 5))\n",
    "        self.line1, = self.ax1.plot([], [], 'r-', label='Loss')\n",
    "        self.line2, = self.ax2.plot([], [], 'b-', label='MAE')\n",
    "        self.line3, = self.ax3.plot([], [], 'g-', label='Huber Loss')\n",
    "        self.ax1.set_xlabel('Epoch')\n",
    "        self.ax1.set_ylabel('Loss')\n",
    "        self.ax2.set_xlabel('Epoch')\n",
    "        self.ax2.set_ylabel('MAE')\n",
    "        self.ax3.set_xlabel('Epoch')\n",
    "        self.ax3.set_ylabel('Huber Loss')\n",
    "        self.ax1.legend()\n",
    "        self.ax2.legend()\n",
    "        self.ax3.legend()\n",
    "        plt.show()\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        self.epochs.append(epoch + 1)\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        self.mae.append(logs.get('mae'))\n",
    "        self.huber.append(logs.get('huber_loss'))\n",
    "        # Update Loss plot\n",
    "        self.line1.set_data(self.epochs, self.losses)\n",
    "        self.ax1.set_xlim(0, max(self.epochs) + 1)\n",
    "        self.ax1.set_ylim(0, max(self.losses) * 1.1)\n",
    "        # Update MAE plot\n",
    "        self.line2.set_data(self.epochs, self.mae)\n",
    "        self.ax2.set_xlim(0, max(self.epochs) + 1)\n",
    "        self.ax2.set_ylim(0, max(self.mae) * 1.1)\n",
    "        # Update Huber Loss plot\n",
    "        self.line3.set_data(self.epochs, self.huber)\n",
    "        self.ax3.set_xlim(0, max(self.epochs) + 1)\n",
    "        self.ax3.set_ylim(0, max(self.huber) * 1.1)\n",
    "        self.figure.canvas.draw()\n",
    "        self.figure.canvas.flush_events()\n",
    "\n",
    "live_plot_callback = LivePlotCallback()\n",
    "\n",
    "# ✅ Step 9: Train the Model with Callbacks and Validation Data\n",
    "model.fit(\n",
    "    train_dataset,\n",
    "    epochs=100,              # Total number of epochs to run (subject to early stopping)\n",
    "    initial_epoch=initial_epoch,\n",
    "    validation_data=val_dataset,\n",
    "    callbacks=[cp_callback, csv_logger, live_plot_callback, early_stop]\n",
    ")\n",
    "\n",
    "# ✅ Step 10: Evaluate the Model on Test Data\n",
    "test_loss, test_mae, test_huber = model.evaluate(test_dataset)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test MAE: {test_mae:.4f}, Test Huber Loss: {test_huber:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
